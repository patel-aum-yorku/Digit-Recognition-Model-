{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1381dd20-d70c-43f6-84a5-19519abbc6d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EECCS 4404 / 5327 Assignment - Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f12dec-ac43-4c06-8b1d-eb6853e9b700",
   "metadata": {},
   "source": [
    "In this exercise, you will implement part of a deep neural network and apply it to the task of hand-written digit recognition. This assignment is adapted from Andrew Ng’s machine learning class on coursera.\n",
    "\n",
    "**NOTE: MAKE SURE YOU RUN ALL THE CODE BLOCK (INCLUDING THE ONES THAT NO CHANGES ARE NEEDED). WHEN YOU MODIFY THE CODE, MAKE SURE YOU RUN IT AFTER THE MODIFICATION.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726b63b-a67d-4ccf-805e-b2847296feac",
   "metadata": {},
   "source": [
    "## Liner Algebra and Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e16c94-c76a-41f8-aec8-62d959f6f9fe",
   "metadata": {},
   "source": [
    "Please review the lecture that we overview linear algebra and the numpy library. If you would like to refresh yourself, please go over relavant lecture notes and the numpy overview Jupyter Notebook. You may also find the [summary of linear algebra](https://minireference.com/static/tutorials/linear_algebra_in_4_pages.pdf), [numpy tutorial](https://www.numpy.org/devdocs/user/quickstart.html), and [numpy documentation](http://www.numpy.org/) helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac55bd-dc15-4f9a-a62c-ff50f3818134",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609fdda-e583-4549-a82d-ef307b902da7",
   "metadata": {},
   "source": [
    "The dataset you will use is taken and modified from the [MNIST digit dataset](http://yann.lecun.com/exdb/mnist/). The dataset consists of 5000 handwritten digit images and the corresponding labels (i.e., correct answers). Each image is 20 pixel by 20 pixel. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The figure below showed some examples from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32d732-fdfb-4b52-bf21-24d985c8ed39",
   "metadata": {},
   "source": [
    "<center><img src=\"img/digit_dataset.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924011f0-49c3-4884-9be8-5f3633b957e7",
   "metadata": {},
   "source": [
    "Please answer the following question (not in this Jupyter file, but in the designated places in **eClass**)\n",
    "\n",
    "**Q1.** What kind of problem is this?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Supervised Learning: Regression</li>\n",
    "  <li>Supervised Learning: Classification</li>\n",
    "  <li>Unsupervised Learning: Clustering</li>\n",
    "  <li>Unsupervised Learning: Dimension Reduction</li>\n",
    "  <li>Reinforcement Learning</li>\n",
    "</ol>\n",
    "\n",
    "**Q2.** What kind of sub-problem is this?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Binary</li>\n",
    "  <li>Multi-Class</li>\n",
    "  <li>Multi-Label</li>\n",
    "</ol>\n",
    "\n",
    "**Q3.** What kind of training sample would you expect for this dataset?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Relatively balanced</li>\n",
    "  <li>skewed dataset</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d27542-7d8c-4133-a2b1-79a230a65cbc",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394da95-af83-4c25-9a4c-9e34aca784fa",
   "metadata": {},
   "source": [
    "Here is a basic unit/node of a neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd448d-621c-4eeb-965b-88dc7c253ea0",
   "metadata": {},
   "source": [
    "<center><img src=\"img/neuron.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80c329-46a7-479b-8462-f35ca0f4b1c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "It takes weighted inputs. In this example, $z = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$ (assume $w_1$, $w_2$, and $w_3$ are the weights correspondingly, $b$ is the bias which didn’t show in the figure above). The output is $a = g(z)$ where $g$ is a non-linear activation function. In this assignment, we use the sigmoid function $\\sigma(z)$ which is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b1041a-44f7-4338-8b29-b927ef9a4acb",
   "metadata": {},
   "source": [
    "<center>$\\sigma(z)=\\frac{1}{1+e^{-z}}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c17a8-c03a-41e0-a3bf-61b7f5b4b8fc",
   "metadata": {},
   "source": [
    "A sigmoid function looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17263553-3322-4b1e-a7a1-c419b0953714",
   "metadata": {},
   "source": [
    "<center><img src=\"img/sigmoid.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3105bc-e360-4831-8719-41919b3c7b61",
   "metadata": {},
   "source": [
    "A neural network is composed of these units. There are many such units in each layer, and there are many layers. In this assignment, we will firstly use a 2-layers neural net. The layers are an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. The size of each image is 20 by 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c961eb-122d-4e9d-945d-fbc5d3315ec5",
   "metadata": {},
   "source": [
    "The training data will be loaded into the variables `train_x` and `train_y` by the function `load_data(training_percentage)`. Let's set the `training_percentage` to 1. You can ignore the `test_x` and `test_y` generated by the load_data function for now. \n",
    "The `train_x` contains 5000 vectorized samples, and the `train_y` stores the corresponding labels like 6, 1, 2, etc. Please run the following code as is.\n",
    "\n",
    "Optional: You can play around with different values for `training_percentage` to see the result. This parameter indicates the percentage of data used for training data, and the rest will be used as testing data. So the range is between 0 and 1. But when you submit this file, make sure it is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f8f4c-772e-4abb-b8c2-5088b75566bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "\n",
    "from util import load_data, display_digit_image, reshape_Y, display_cost, compute_accuracy\n",
    "\n",
    "# DO NOT CHANGE ANY PARAMETERS BELOW! \n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "np.random.seed(3)\n",
    "\n",
    "training_percentage = 1\n",
    "\n",
    "train_x, train_y, test_x, test_y = load_data(training_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fac235-c1ec-4f09-891e-d850128e7b59",
   "metadata": {},
   "source": [
    "To visualize a sample from the training examples, please run the following code. Feel free to change `index` to see another training example (e.g., 2) (Note: this number is not the label, just the index of the training example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb40e85-c5b5-4e8d-b6d8-497212b8d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digit_image(train_x, train_y, index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93281bab-db9c-4156-9251-48df78678a60",
   "metadata": {},
   "source": [
    "For the hidden layer, we will first include 25 units, and we will play around it later. For the output layer, we will set it to have 10 units. Here is a visualization of the neural network: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8dce4-e896-4d51-8a99-51023608f1ce",
   "metadata": {},
   "source": [
    "<center><img src=\"img/neural_network.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f20366-642a-41b5-b76b-3aead4e9ea91",
   "metadata": {},
   "source": [
    "Please answer the following question (not in this Jupyter file, but in **eClass**)\n",
    "\n",
    "**Q4.** Why are there $400$ units in the input layer?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Because the images are of size $20$ by $20$. Each node corresponding to a pixel in the image. This results in $400$ input layer units (not counting the extra bias unit which always outputs $1$)</li>\n",
    "  <li>Because the first layer need to include a lot of neurons so we choose $400$</li>\n",
    "  <li>The number $400$ is generated randomly, and it can be any number here</li>\n",
    "</ol>\n",
    "\n",
    "**Q5.** Why are there $25$ units in the hidden layer?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Because $25$ can be divided by $400 \\times 10$</li>\n",
    "  <li>Because $25$ share common divisor of integers with $400$ and $10$ (e.g., $5$)</li>\n",
    "  <li>The layer does not have to have exactly 25 units. It can be another number such as 24, 26, 27, 29, etc. </li>\n",
    "</ol>\n",
    "\n",
    "**Q6.** Why are there $10$ units in the output layer?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Because $10$ can be divided by $400 \\times 25$</li>\n",
    "  <li>Because there are $10$ labels corresponding to $10$ digits(i.e., 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)</li>\n",
    "  <li>The number $10$ is generated randomly, and it can be any number here</li>\n",
    "</ol>\n",
    "\n",
    "**Q7.** The activation function in the hidden layer will use the sigmoid function. In class, we mentioned that the activation function of the output layer is dependent on the type of problem. Which activation function is appropriate for classification problems? (check all that apply).\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>Linear activation</li>\n",
    "  <li>Sigmoid function</li>\n",
    "  <li>ReLU function</li>\n",
    "  <li>Leaky ReLU function</li>\n",
    "  <li>tanh function</li>\n",
    "  <li>Softmax function</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c9c62-7be7-4b22-b04b-633408c1fb7c",
   "metadata": {},
   "source": [
    "The value provided to the input layer will be weighted and passed to the hidden layer. The value at the hidden layer will then go through the activation function and the result will be passed to the output layer. The value will also go through the activation function. At the output layer, the node of the largest value will win and the label represented by the node will be the image’s label. This process is called **feedforward**, and process can be used to **predict** the label given new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249fac9-80f5-4cf2-9c0c-c391859ff658",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will initialize the weights randomly. the output will be very different from the true label. The cost function is used to evaluate how different it is between the true label and the predicted label. The network will then propagate this information back by calculating the derivatives. The weights of the network will update themselves accordingly. This process is called **backpropagation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab00867-8398-4a55-8c21-9f9c5e7e054c",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61637e1f-e788-489b-8b9f-952c0257623d",
   "metadata": {},
   "source": [
    "The input data and output can be obtained with (feel free to add code to check the shape of these values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf046802-4d66-4431-9f82-8dd93830df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_x\n",
    "y = train_y\n",
    "Y = reshape_Y(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fabf4a-b0dc-4936-852a-e51aa3491bb2",
   "metadata": {},
   "source": [
    "The shape is in the form of row by column. For example, for the following matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcab91b-7f3d-4de0-8ad6-71d89e851e11",
   "metadata": {},
   "source": [
    "<center>$\\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a8be6-400c-4002-9bf8-77c4ab0ad2c8",
   "metadata": {},
   "source": [
    "It can be defined in numpy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30388795-1460-4e6f-89a4-645c945705a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ed4b5-5b17-496d-b728-706d1f2d8593",
   "metadata": {},
   "source": [
    "It is a 2 by 3 matrix, and the shape can be obtained with `a.shape`. The result can be printed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdce74-0494-4114-9918-b7826fda1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506374b-a5ed-40f6-9a08-69a9120bcbc0",
   "metadata": {},
   "source": [
    "Before implementing the feedforward process, think about the dimension of the parameters and values. Correct dimensions can help you a lot when debugging the code. Recall that the neural network we will implement is with 3 layers, there are 400 units in the input layer, 25 in the hidden layer, and 10 at the output layer.\n",
    "\n",
    "Note: It is very important to know the dimensions!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539cd766-a41c-4c4e-9eca-28c47d2c9fcc",
   "metadata": {},
   "source": [
    "**Q8-Q23**. Please fill in the dimension\n",
    "|values|number of rows|number of columns|\n",
    "|:----:|:-------:|:-------:|\n",
    "| X (input, which is `train_x)` |         |         |\n",
    "| y (true label, which is `train_y`) |         |         |\n",
    "| Y (true label, which is the reshaped `train_y`) |         |         |\n",
    "| A0 |         |         |\n",
    "| Z1 |         |         |\n",
    "| A1 |         |         |\n",
    "| Z2 |         |         |\n",
    "| A2 |         |         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df04b02-4417-4417-a57a-6cb5edbf01a9",
   "metadata": {},
   "source": [
    "Please fill free to print out the shape of `X`, `y`, and `Y`. The index represents which layer the value belongs to. For example, `Z2` is the linear combination of the values from the values of the previous layer, and `A2` is the value after activation (we choose sigmoid in this assignment). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411ed20-2c7d-4a9f-b931-2d1b6e8977de",
   "metadata": {},
   "source": [
    "Here you can see that the dimension of `Y` and `y` is different. This is because the raw data provided 1, 2, 3, 4 as labels, and the output of the neural net uses one-hot encoding and it is a vector of length 10, with index 0 corresponds to the probability of digit 0, index 1 corresponds to the probability of digit 1, etc. Based on the information provided here and above, please answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2620c4-8ead-42e0-bdee-07cc1a1680d6",
   "metadata": {},
   "source": [
    "**Q24.** Given this converted label:\n",
    "<center>$\\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\0\\\\1\\\\0\\\\0\\\\0\\\\0\\end{bmatrix}$</center>\n",
    "From 0 to 9, which digit is this label corresponding to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0d588-9aea-41c2-ab5d-dc62636764fb",
   "metadata": {},
   "source": [
    "**Q25.** Given this output:\n",
    "<center>$\\begin{bmatrix}0.02\\\\0.06\\\\0.9\\\\0.0004\\\\0.1\\\\0.025\\\\0.062\\\\0.3\\\\0.1\\\\0.12\\end{bmatrix}$</center>\n",
    "which digit is this image mostly represents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2d02f-70e5-44c4-a2a7-cd5d379aef37",
   "metadata": {},
   "source": [
    "**Q26.**  True or False? The dimensions of `A0` and `X` are the same. This is a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b623a-0c58-4bba-95b2-b90ba76cd20a",
   "metadata": {},
   "source": [
    "**Q27.** True or False? The dimensions of `A2` and `Y` are the same. This is a coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ea0c2-2865-47e7-bccb-f8697b0b959f",
   "metadata": {},
   "source": [
    "Now let's take a look at the implementation. Let's initialize the neural network with the following hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94efc6f-81b9-4a57-bbe9-c9fc7ac4554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [400, 25, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfda7a-7be0-4dac-9dac-3ee369fdcfc3",
   "metadata": {},
   "source": [
    "It represents a two-layer neural network. The first layer (i.e., the input layer) has 400 units, the second layer (i.e., the hidden layer) has 25 units, and the third layer (i.e., the output layer) has 10 units. The `initialize_parameters` function initilize the parameters of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e717295-b5ba-48e8-b52b-2279706d802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a54ef-2e36-4a72-ba4d-0a088e872320",
   "metadata": {},
   "source": [
    "It returns a dictionary of parameters, with the keywords `W1`, `b1`, `W2` and `b2`. The `W*` represents the weights, and the `b*` is the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee2599-4fd3-49b7-b2f9-cfe0d03bce46",
   "metadata": {},
   "source": [
    "Given the input `A_prev` from the previous layer, and the parameters `W` and `b`, the values at the current layer is calculated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f8c92-9c1e-41ef-a746-9fe915c216ae",
   "metadata": {},
   "source": [
    "$$Z = WA_{prev} + b$$\n",
    "$$A = sigmoid(Z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251955f-ee39-437e-b915-92fcbaa59f2b",
   "metadata": {},
   "source": [
    "Please complete the sigmoid function (please put your code in between the start code and end code here, and delete the `pass`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a44a7-0647-4cf0-b094-9c7d04ef86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b1174-b015-4cc0-8fe5-2fa8f1471127",
   "metadata": {},
   "source": [
    "You can check the correctness of your implementation by visualizing the function with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39246f8f-fe7c-4563-b4cf-90facb5df3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_x = np.arange(-11., 11.)\n",
    "sigmoid_y = sigmoid(sigmoid_x)\n",
    "plt.plot(sigmoid_x, sigmoid_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884539b-cc7e-42de-9f8f-62bedc79e39f",
   "metadata": {},
   "source": [
    "Please complete the sigmoid function (please put your code in between the start code and end code here). *Hint: you can use the `sigmoid` function you implemented.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e4b9e-02f6-4087-b945-93b2dac802e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for one layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the result of linear calculation\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###    \n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7912dac-b656-489f-bc2e-2069e0a24bee",
   "metadata": {},
   "source": [
    "Here is the `feedforward` function and the `predict` function that is implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b09cb-b361-4ac5-8b6e-0a024b96f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation. The activation functions are all sigmoid functions.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters(layer_dims)\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- a python dictionary containing \"a1\", \"z2\", \"a2\", \"z3\", ...; they are stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    caches = {}\n",
    "    caches[\"A0\"] = X\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers (excluding input layer, including output layer) in the neural network\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        A_prev = A \n",
    "        Z, A = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        caches[\"Z\" + str(l)] = Z\n",
    "        caches[\"A\" + str(l)] = A\n",
    "    \n",
    "    assert(A.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106c1c7-ffae-4968-9a6c-14780dc94370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (0, 1, 2, 3 ...)\n",
    "    \"\"\"\n",
    "    A2, cache = feedforward(X, parameters)\n",
    "    predictions = np.argmax(A2, axis = 0)\n",
    "    np.reshape(predictions, (1, np.shape(predictions)[0]))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827c737-dc48-44f8-a854-13c7cccb38ef",
   "metadata": {},
   "source": [
    "The values are cached so that the calculations in the backpropogation process can be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ece51-1b0d-45bb-aeb0-be81d45ea792",
   "metadata": {},
   "source": [
    "If you implement the above code correctly, this following chunk of code should output `[0.49058414 0.49043377 0.49029028 ... 0.49059123 0.49059543 0.49074616]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295468e-bd9f-4633-b454-1307a7c6b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feedforward(train_x, initialize_parameters(layers_dims))[1][\"A2\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024a0e6-0d8a-4e49-ba52-d2135dbf3861",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760863aa-57be-4532-a9f8-5a409a20d6a8",
   "metadata": {},
   "source": [
    "The cost function is calculated as follows (this might look a little bit different to the vectorization form of logistic regression we have in the class. This is because the output in logistic regression is a vector as we only expect 1 predicted value in each example, but it is a matrix in neural network):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48b53a-6c20-4193-ad6a-a39cc7731d3c",
   "metadata": {},
   "source": [
    "<center>$$J(W, b)=\\sum_{n=1}^{m} \\frac{1}{m}[-Y \\circ \\log(AL)-(1-Y) \\circ \\log(1-AL)]$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37998d1-1104-498b-881a-f736c5f9be23",
   "metadata": {},
   "source": [
    "where `AL` is the value of the output layer (in our neural network, it is `A2`), $\\circ$ is element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf2966-142d-4f8d-853e-da5a6e9b4ec6",
   "metadata": {},
   "source": [
    "Please implement the cost function (please put your code in between the start code and end code here, and delete the `pass`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6b5c2-7b89-4099-9898-20e3d65f08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (10, number of examples)\n",
    "    Y -- true \"label\" vector after reshape by reshape_Y function, shape (10, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute cost.\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###  \n",
    "\n",
    "    cost = np.squeeze(cost)     # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabad03-1cf8-4e76-8692-2de764f94336",
   "metadata": {},
   "source": [
    "Here is how to update the weights. The main idea is to calculate the “differences” (which is actually derivative. You can perceive it as a way to evaluate as difference, but it is not just a simple subtraction in quantity) of the current value to the target value (which is denoted as `dZ*`, where * is the layer index), and thus figure out the differences (denoted as `dW*` and `db*`)  between the current weights and the ideal value of weights, and update the weights accordingly. And we will work backwards, which is from the output layer to the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f0286-b8b7-49fc-a588-6c4c854673cc",
   "metadata": {},
   "source": [
    "The weight and bias at each layer are updated with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e427d4-e375-4c7c-88e5-bfcea2ec9da7",
   "metadata": {},
   "source": [
    "<center>$$W_i=W_i-\\alpha \\frac{\\partial J}{\\partial W_i}$$</center>\n",
    "<center>$$b_i=b_i-\\alpha \\frac{\\partial J}{\\partial b_i}$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729fe72-b6bc-4b53-8af7-0f1b5e663637",
   "metadata": {},
   "source": [
    "where $i$ is the layer index, and $\\alpha$ is the learning rate. Here are the formulas to calculate the partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d886cf2-229e-4a65-afa9-622834f619af",
   "metadata": {},
   "source": [
    "<center>$$\\frac{\\partial J}{\\partial W_i} =\\frac{1}{m} \\frac{\\partial J}{\\partial Z_{i}} A_{i-1}^T$$</center>\n",
    "<center>$$\\frac{\\partial J}{\\partial b_i} =\\frac{1}{m} \\frac{\\partial J}{\\partial Z_{i}} \\vec{1}$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdad8fa-7f21-432e-a3ff-c5e3325e9212",
   "metadata": {
    "tags": []
   },
   "source": [
    "The partial derivative with respect to $b_i$ simply get the mean of each row of $\\frac{\\partial J}{\\partial Z_{i}}$, and there could be more than one way to achieve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae8944-2043-4fbc-9c57-0cbb4ecefcdd",
   "metadata": {},
   "source": [
    "**Q28.** In the formula of $\\frac{\\partial J}{\\partial W_i}$, what is the operation between $\\frac{1}{m}$ and $\\frac{\\partial J}{\\partial Z_i} A_{i-1}^T$? *Hint: $\\frac{1}{m}$ is a scalar, and $\\frac{\\partial J}{\\partial Z_i} A_{i-1}^T$ is a matrix*\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>scalar multiplication</li>\n",
    "  <li>matrix multiplication (multiplication between two matrices/vectors)</li>\n",
    "  <li>element-wise multiplication</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d230af-b5ed-4f78-9131-9639743e5372",
   "metadata": {},
   "source": [
    "**Q29.** In the formula of $\\frac{\\partial J}{\\partial W_i}$, what is the operation between $\\frac{1}{m} \\frac{\\partial J}{\\partial Z_i}$ and $A_{i-1}^T$? *Hint: $\\frac{1}{m} \\frac{\\partial J}{\\partial Z_i}$ is a matrix, and $A_{i-1}^T$ is also a matrix*\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>scalar multiplication</li>\n",
    "  <li>matrix multiplication (multiplication between two matrices/vectors)</li>\n",
    "  <li>element-wise multiplication</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5882a0-5a27-44aa-b89c-bbdda7dc3766",
   "metadata": {},
   "source": [
    "**Q30.** In the formula of $\\frac{\\partial J}{\\partial b_i}$, what is the operation between $\\frac{1}{m}$ and $\\frac{\\partial J}{\\partial Z_i} \\vec{1}$? *Hint: $\\frac{1}{m}$ is a scalar, and $\\frac{\\partial J}{\\partial Z_i} \\vec{1}$ is also a scalar*\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>scalar multiplication</li>\n",
    "  <li>matrix multiplication (multiplication between two matrices/vectors)</li>\n",
    "  <li>element-wise multiplication</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a2dfb-cb1b-43f5-a559-ed23afaf9edb",
   "metadata": {},
   "source": [
    "**Q31.** In the formula of $\\frac{\\partial J}{\\partial b_i}$, what is the operation between $\\frac{1}{m} \\frac{\\partial J}{\\partial Z_i}$ and $\\vec{1}$? *Hint: $\\frac{1}{m} \\frac{\\partial J}{\\partial Z_i}$ is a matrix, and $\\vec{1}$ is also a vector*\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>scalar multiplication</li>\n",
    "  <li>matrix multiplication (multiplication between two matrices/vectors)</li>\n",
    "  <li>element-wise multiplication</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a11e1a-4e45-427f-9691-60b0f625b199",
   "metadata": {},
   "source": [
    "In the partial derivative formulas, you may observe that the key is to find:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f9a92-54fc-4ce4-82ea-05f69255cccd",
   "metadata": {},
   "source": [
    "<center>$$\\frac{\\partial J}{\\partial Z_i}$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f768b-4264-4925-b321-d5b287b58f26",
   "metadata": {},
   "source": [
    "At the output layer, the $\\frac{\\partial{J}}{\\partial{Z_2}}$ is calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952765f4-ed4c-43ea-b13c-d013b8028354",
   "metadata": {},
   "source": [
    "<center>$$\\frac{\\partial{J}}{\\partial{Z_2}}=A_2-Y$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3aa5bb-505c-4d5d-96b5-6e31c5e47ea6",
   "metadata": {},
   "source": [
    "At the hidden layer, $\\frac{\\partial{J}}{\\partial{Z_1}}$ are calculated as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5016f87-283f-4814-898b-3bfaf0780058",
   "metadata": {},
   "source": [
    "<center>$$\\frac{\\partial{J}}{\\partial{Z_1}}=(W_2^T\\frac{\\partial{J}}{\\partial{Z_2}}) \\circ \\frac{\\partial{sigmoid(Z_1)}}{\\partial{Z_1}}$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07712a61-bfd8-4d0d-8350-91522c1e90a9",
   "metadata": {},
   "source": [
    "where $\\circ$ represents element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e0300-80fb-4f9f-997b-7c4a64cd615a",
   "metadata": {},
   "source": [
    "**Q32.** Notice that we did not mention $\\frac{\\partial{J}}{\\partial{Z_0}}$, why is that (you can assume $Z_0$ is $X$)?\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_0}}$ should exist, but it is just not used in the calculation later. So this value is excluded</li>\n",
    "  <li>The calculation of $\\frac{\\partial{J}}{\\partial{Z_0}}$ is the same as $\\frac{\\partial{J}}{\\partial{Z_1}}$. There is no need to repeat.</li>\n",
    "  <li>If we are going to calculate $\\frac{\\partial{J}}{\\partial{Z_0}}$, that will be the difference between the desired value and the actual value we got. The actual value is the image, and we cannot modify the input. So there is no need to calculate $\\frac{\\partial{J}}{\\partial{Z_0}}$</li>\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_0}}$ should be there, and the calculation is different from $\\frac{\\partial{J}}{\\partial{Z_1}}$. The course staff made a mistake and the formula should be there.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375356c9-1c50-4e15-91f4-91d334e55e7d",
   "metadata": {},
   "source": [
    "Now let's generalize the formula to neural network with artibrary number of layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b626e-437c-44bd-a027-3f52a07921fb",
   "metadata": {},
   "source": [
    "**Q33.** For the output layer of an n-layer neural network:\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_n}}=A_n-Y$</li>\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_n}}=Y+A_n$</li>\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_n}}=Y-A_n$</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3b601-a74e-4069-aba1-db4148ab94d6",
   "metadata": {},
   "source": [
    "**Q34.** For the hidden layer of an n-layer neural network:\n",
    "<ol style=\"list-style-type: upper-alpha\">\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_i}}=(W_{i+1}^T\\frac{\\partial{J}}{\\partial{Z_{i+1}}}) \\circ \\frac{\\partial{sigmoid(Z_i)}}{\\partial{Z_i}}$</li>\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_i}}=(W_i^T\\frac{\\partial{J}}{\\partial{Z_{i}}}) \\circ \\frac{\\partial{sigmoid(Z_i)}}{\\partial{Z_i}}$</li>\n",
    "  <li>$\\frac{\\partial{J}}{\\partial{Z_i}}=(W_{i+1}^T\\frac{\\partial{J}}{\\partial{Z_{i+1}}}) \\circ \\frac{\\partial{sigmoid(Z_{i+1})}}{\\partial{Z_{i+1}}}$</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c91e7-4f30-4744-9025-2d602b780c6f",
   "metadata": {},
   "source": [
    "Now let's implement backpropagation. Note that in the program, we represent $\\frac{\\partial{J}}{\\partial{Z_i}}$ as `dZi`, $\\frac{\\partial{J}}{\\partial{W_i}}$ as `dWi`. First, let's implement the gradient of a sigmoid function. Note that $\\frac{\\partial{sigmoid(Z_i)}}{\\partial{Z_i}}=sigmoid(Z_i) \\circ (1-sigmoid(Z_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e6ab6-49e8-42fb-a6a0-66be0bd8f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(Z):\n",
    "    \"\"\"\n",
    "    Implements the inverse of the sigmoid\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array\n",
    "    \n",
    "    Returns:\n",
    "     -- output of inverse of sigmoid(Z), same shape as Z\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b498cc-9217-4f3a-822e-e0662b8bc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(AL, Y, parameters, caches, layers_dims):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation. This is the function that calculates the partial derivatives dWi. This function also caches dZi in the dictionary grad.\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation\n",
    "    Y -- true \"label\" vector after reshaping by reshape_Y\n",
    "    caches -- output of the feedforward function\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dZ\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(layers_dims)\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    # Initializing the backpropagation, by calculating the dZi of the last layer and save it to grads with the key \"dZ*\" where * is the index of the layer\n",
    "    # pleaes do not hard code to 3 here \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65ef5c-5eeb-4516-84f0-ec610a26f72b",
   "metadata": {},
   "source": [
    "Note: It is **very likely** that you make a mistake in the code below. Be very careful of the range of the loop. If you set the range wrong, you will not be able to get a trained model with low cost (e.g., less than 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ea09b-558a-46e3-95c6-9907d4a36876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of backpropagation\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "                  \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2f799-88fd-4cf1-9c5d-b5a9c2181d32",
   "metadata": {},
   "source": [
    "## Put it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d670e-96e4-400f-aeb1-eaf2a9d8c9ae",
   "metadata": {},
   "source": [
    "Implement the required lines below and put together the functions you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a689b-513c-43ee-8312-58939e3e5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_NN(X, Y, layers_dims, learning_rate, num_iterations, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->SIGMOID]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector, of shape (10, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints all the cost\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    costs = []                         # keep track of the cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation:\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        if print_cost:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "        costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6448df-db49-42a2-81b6-8bdca62ed22a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's train the neural network by the the code below. NOTE: the training do need some time. If you what know which iteration it is as, you can set `print_cost=True`, but do not modify the code otherwise.\n",
    "\n",
    "Optional: you can try different values of `training_percentage`, `learning_rate`, `num_iterations`, or even using a neural network with a different structure by modifying `layers_dims` after you complete the assignment and see whether you will get a more accurate modell, or less accurate model. You are very welcome to share your finding on the discussion forum. But when you submit your assignment, please submit it with the default values:\n",
    "```\n",
    "training_percentage = 1\n",
    "layers_dims = [400, 25, 10]\n",
    "learning_rate = 0.5\n",
    "num_iterations = 2000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2544adc-d8ad-4b98-b6b2-3814093c155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "training_percentage = 1\n",
    "layers_dims = [400, 25, 10]\n",
    "learning_rate = 0.5\n",
    "num_iterations = 2000\n",
    "\n",
    "# load data and train\n",
    "train_x, train_y, test_x, test_y = load_data(training_percentage) # load again just in case it got modified while you play around with the code\n",
    "parameters, costs = deep_NN(train_x, reshape_Y(train_y), layers_dims, learning_rate, num_iterations, print_cost=False)\n",
    "\n",
    "# show results\n",
    "display_cost(costs, learning_rate, figure_index=2)\n",
    "train_set_predictions = predict(parameters, train_x)\n",
    "train_set_accuracy = compute_accuracy(train_set_predictions, train_y)\n",
    "print(\"the accuracy on the training set is:\", train_set_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe57a38-084a-4ee9-bf0c-e30209628d9c",
   "metadata": {},
   "source": [
    "**Q35.** What is the accuracy you obtained above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a6b85-7fc1-4fe5-88c2-28e72751e5cc",
   "metadata": {},
   "source": [
    "**Q36.** Please copy and paste your figure (or take a screenshot) to the answersheet using \"Insert or edit image\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53fee0-d800-441b-857a-6648558b4347",
   "metadata": {},
   "source": [
    "**Q37**. Please submit the Jupyter notebook that you have completed. Do not clear the output! To get the Jupyter notebook, Click File -> Download. To upload the jupyter file, you can drag and drop it to the box in the answer sheet in eClass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
